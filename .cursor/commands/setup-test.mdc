---
description: Generate a test setup for dialectic debate testing with test scripts that can run against any problem
---

# Setup Test Infrastructure

Generate a test setup for dialectic debate testing. This command creates a new test directory in `e2e-tests/` with test scripts that can be run against any problem. Tests are independent and reusable across all problems in the `examples/` directory.

## Input Parameters

The command accepts the following parameters:

- **test_name** (required)
  - The name of the test directory to create (e.g., `my_test`, `custom_rounds_test`, `performance_test`)
  - Will be created as `e2e-tests/{test_name}/`
  - Must not start with underscore (underscore-prefixed directories are skipped by the test runner)
  - Must be a valid directory name (no special characters that would cause filesystem issues)

- **test_type** (optional, default: `rounds_test`)
  - Type of test pattern to generate:
    - `rounds_test` - Tests debates with different numbers of rounds (1-5)
    - `clarify_test` - Tests debates with and without the clarification phase
    - `custom` - Custom test pattern (requires user to provide script logic in fenced code block)
  - If `custom` is selected, the user must provide the script logic for `run_test.sh` in a fenced code block

- **requires_test_configs** (optional, default: `false`)
  - Whether the test needs test-specific configuration files
  - If `true`, the test will require `TEST_DIR` to be provided (mandatory third argument)
  - If `false`, the test will use problem default configs (TEST_DIR is optional)

- **model** (optional, only used if `requires_test_configs` is `true`)
  - The LLM model identifier to use for test-specific configurations
  - Format depends on provider (e.g., `google/gemini-2.5-flash-lite` for OpenRouter, `gpt-4` for OpenAI)

- **provider** (optional, only used if `requires_test_configs` is `true`, default: `openrouter`)
  - The LLM provider to use (`openai` or `openrouter`)
  - Only used when creating test-specific configuration files

## Generated Files

The command will create the following files:

### 1. `e2e-tests/{test_name}/run_test.sh`
Test execution script that:
- Accepts three arguments: `PROBLEM_DIR`, `OUTPUT_DIR`, `TEST_DIR` (TEST_DIR is optional unless `requires_test_configs` is true)
- Creates the output directory if it doesn't exist
- Runs debate commands based on the test type
- Uses config resolution: test-specific configs if available, otherwise problem default configs
- Includes shebang (`#!/bin/bash`) and executable permissions

### 2. `e2e-tests/{test_name}/eval_run.sh`
Evaluation script that:
- Accepts three arguments: `PROBLEM_DIR`, `OUTPUT_DIR`, `TEST_DIR` (TEST_DIR is optional)
- Evaluates all debate outputs from the test run
- Uses config resolution: test-specific eval config if available, otherwise problem default eval config
- Writes evaluation results to the output directory
- Includes shebang and executable permissions

### 3. `e2e-tests/{test_name}/debate-config.json` (optional, if `requires_test_configs` is `true`)
Test-specific debate configuration file (only created if `requires_test_configs` is `true`).

## Implementation Instructions

When executing this command, follow these steps precisely:

1. **Validate Test Name**
   - Check that `test_name` is provided and non-empty
   - Validate that `test_name` doesn't start with underscore (test runner skips these)
   - Check that `e2e-tests/{test_name}` doesn't already exist (warn if it does, but proceed)
   - Ensure `test_name` is a valid directory name

2. **Create Test Directory Structure**
   - Create directory `e2e-tests/{test_name}/` if it doesn't exist
   - Ensure parent directories exist

3. **Determine Test Type and Generate Scripts**
   - Based on `test_type`, generate appropriate `run_test.sh` script:
     - `rounds_test`: Generate script that runs debates with rounds 1-5
     - `clarify_test`: Generate script that runs debates with and without `--clarify` flag
     - `custom`: Extract script logic from fenced code block provided by user

4. **Create run_test.sh Script**
   - Create `e2e-tests/{test_name}/run_test.sh`
   - Use the new 3-argument pattern:
     - `PROBLEM_DIR="$1"` (required)
     - `OUTPUT_DIR="$2"` (required)
     - `TEST_DIR="${3:-}"` (optional, unless `requires_test_configs` is true)
   - If `requires_test_configs` is `true`, make TEST_DIR mandatory (check for `$3` and exit if missing)
   - Add config resolution logic:
     ```bash
     if [ -n "$TEST_DIR" ] && [ -f "$TEST_DIR/debate-config.json" ]; then
       CONFIG="$TEST_DIR/debate-config.json"
     else
       CONFIG="$PROBLEM_DIR/debate-config.json"
     fi
     ```
   - Generate debate commands based on test type
   - Add shebang: `#!/bin/bash`
   - Add error checking: exit if required arguments are missing
   - Make script executable (set file permissions)

5. **Create eval_run.sh Script**
   - Create `e2e-tests/{test_name}/eval_run.sh`
   - Use the same 3-argument pattern as `run_test.sh`
   - Add config resolution logic for eval config:
     ```bash
     if [ -n "$TEST_DIR" ] && [ -f "$TEST_DIR/eval_config.json" ]; then
       EVAL_CONFIG="$TEST_DIR/eval_config.json"
     else
       EVAL_CONFIG="$PROBLEM_DIR/eval_config.json"
     fi
     ```
   - Generate eval commands for all debate outputs created by `run_test.sh`
   - Add shebang: `#!/bin/bash`
   - Add error checking: exit if required arguments are missing
   - Make script executable (set file permissions)

6. **Create Test-Specific Configs (if required)**
   - If `requires_test_configs` is `true`:
     - Create `e2e-tests/{test_name}/debate-config.json`
     - Configure with the specified model and provider
     - Use the same structure as problem default configs (4 agents, judge, debate settings)

7. **Path Handling**
   - All paths in scripts should use forward slashes
   - Scripts receive absolute paths from `e2e-tests/run-tests.ts`
   - Use variables consistently throughout scripts
   - Ensure paths are properly quoted in bash scripts

8. **File Permissions**
   - Add `#!/bin/bash` shebang to both scripts
   - Set executable permissions on both shell scripts (chmod +x equivalent)
   - Scripts should work in both WSL and Git Bash environments

## Test Type Templates

### rounds_test Template

```bash
#!/bin/bash

# Check if required arguments are provided
if [ -z "$1" ] || [ -z "$2" ]; then
    echo "Error: Problem directory and output directory arguments are required" >&2
    echo "Usage: $0 <problem_dir> <output_dir> [test_dir]" >&2
    exit 1
fi

PROBLEM_DIR="$1"
OUTPUT_DIR="$2"
TEST_DIR="${3:-}"  # Optional third parameter

# Ensure output directory exists
mkdir -p "$OUTPUT_DIR"

# Use test-specific config if TEST_DIR provided and config exists, otherwise use problem config
if [ -n "$TEST_DIR" ] && [ -f "$TEST_DIR/debate-config.json" ]; then
  CONFIG="$TEST_DIR/debate-config.json"
else
  CONFIG="$PROBLEM_DIR/debate-config.json"
fi

# Run debates with rounds 1-5
dialectic debate -r 1 -c "$CONFIG" -o "$OUTPUT_DIR/all_agents_1R_no_clarify.json" -p "$PROBLEM_DIR/problem.md" -v
dialectic debate -r 2 -c "$CONFIG" -o "$OUTPUT_DIR/all_agents_2R_no_clarify.json" -p "$PROBLEM_DIR/problem.md" -v
dialectic debate -r 3 -c "$CONFIG" -o "$OUTPUT_DIR/all_agents_3R_no_clarify.json" -p "$PROBLEM_DIR/problem.md" -v
dialectic debate -r 4 -c "$CONFIG" -o "$OUTPUT_DIR/all_agents_4R_no_clarify.json" -p "$PROBLEM_DIR/problem.md" -v
dialectic debate -r 5 -c "$CONFIG" -o "$OUTPUT_DIR/all_agents_5R_no_clarify.json" -p "$PROBLEM_DIR/problem.md" -v
```

### clarify_test Template

```bash
#!/bin/bash

# Check if required arguments are provided
if [ -z "$1" ] || [ -z "$2" ]; then
    echo "Error: Problem directory and output directory arguments are required" >&2
    echo "Usage: $0 <problem_dir> <output_dir> [test_dir]" >&2
    exit 1
fi

PROBLEM_DIR="$1"
OUTPUT_DIR="$2"
TEST_DIR="${3:-}"  # Optional third parameter

# Ensure output directory exists
mkdir -p "$OUTPUT_DIR"

# Use test-specific config if TEST_DIR provided and config exists, otherwise use problem config
if [ -n "$TEST_DIR" ] && [ -f "$TEST_DIR/debate-config.json" ]; then
  CONFIG="$TEST_DIR/debate-config.json"
else
  CONFIG="$PROBLEM_DIR/debate-config.json"
fi

# Run debates with and without clarify
dialectic debate -r 2 -c "$CONFIG" -o "$OUTPUT_DIR/debate-with-clarify.json" -p "$PROBLEM_DIR/problem.md" -v --clarify
dialectic debate -r 2 -c "$CONFIG" -o "$OUTPUT_DIR/debate-without-clarify.json" -p "$PROBLEM_DIR/problem.md" -v
```

### eval_run.sh Template (for rounds_test)

```bash
#!/bin/bash

# Check if required arguments are provided
if [ -z "$1" ] || [ -z "$2" ]; then
    echo "Error: Problem directory and output directory arguments are required" >&2
    echo "Usage: $0 <problem_dir> <output_dir> [test_dir]" >&2
    exit 1
fi

PROBLEM_DIR="$1"
OUTPUT_DIR="$2"
TEST_DIR="${3:-}"  # Optional third parameter

# Ensure output directory exists
mkdir -p "$OUTPUT_DIR"

# Use test-specific eval config if TEST_DIR provided and config exists, otherwise use problem config
if [ -n "$TEST_DIR" ] && [ -f "$TEST_DIR/eval_config.json" ]; then
  EVAL_CONFIG="$TEST_DIR/eval_config.json"
else
  EVAL_CONFIG="$PROBLEM_DIR/eval_config.json"
fi

# Run evaluations for all debate outputs
dialectic eval -c "$EVAL_CONFIG" -d "$OUTPUT_DIR/all_agents_1R_no_clarify.json" -v -o "$OUTPUT_DIR/eval2_all_agents_1R_no_clarify.eval.json"
dialectic eval -c "$EVAL_CONFIG" -d "$OUTPUT_DIR/all_agents_2R_no_clarify.json" -v -o "$OUTPUT_DIR/eval2_all_agents_2R_no_clarify.eval.json"
dialectic eval -c "$EVAL_CONFIG" -d "$OUTPUT_DIR/all_agents_3R_no_clarify.json" -v -o "$OUTPUT_DIR/eval2_all_agents_3R_no_clarify.eval.json"
dialectic eval -c "$EVAL_CONFIG" -d "$OUTPUT_DIR/all_agents_4R_no_clarify.json" -v -o "$OUTPUT_DIR/eval2_all_agents_4R_no_clarify.eval.json"
dialectic eval -c "$EVAL_CONFIG" -d "$OUTPUT_DIR/all_agents_5R_no_clarify.json" -v -o "$OUTPUT_DIR/eval2_all_agents_5R_no_clarify.eval.json"
```

### eval_run.sh Template (for clarify_test)

```bash
#!/bin/bash

# Check if required arguments are provided
if [ -z "$1" ] || [ -z "$2" ]; then
    echo "Error: Problem directory and output directory arguments are required" >&2
    echo "Usage: $0 <problem_dir> <output_dir> [test_dir]" >&2
    exit 1
fi

PROBLEM_DIR="$1"
OUTPUT_DIR="$2"
TEST_DIR="${3:-}"  # Optional third parameter

# Ensure output directory exists
mkdir -p "$OUTPUT_DIR"

# Use test-specific eval config if TEST_DIR provided and config exists, otherwise use problem config
if [ -n "$TEST_DIR" ] && [ -f "$TEST_DIR/eval_config.json" ]; then
  EVAL_CONFIG="$TEST_DIR/eval_config.json"
else
  EVAL_CONFIG="$PROBLEM_DIR/eval_config.json"
fi

# Run evaluations for all debate outputs
dialectic eval -c "$EVAL_CONFIG" -d "$OUTPUT_DIR/debate-with-clarify.json" -v -o "$OUTPUT_DIR/eval_with-clarify.eval.json"
dialectic eval -c "$EVAL_CONFIG" -d "$OUTPUT_DIR/debate-without-clarify.json" -v -o "$OUTPUT_DIR/eval_without-clarify.eval.json"
```

## Default Values

If optional parameters are not provided, use these defaults:
- `test_type`: `rounds_test`
- `requires_test_configs`: `false`
- `provider`: `openrouter` (only used if `requires_test_configs` is `true`)

**Required parameters** (must be provided):
- `test_name`: The name of the test directory to create

## Verification

After generation:
- All files should be created in the correct locations
- Scripts should have proper shebang and variable usage
- Scripts should be executable
- Scripts should use the new 3-argument pattern (PROBLEM_DIR, OUTPUT_DIR, TEST_DIR)
- The test should be discoverable by `e2e-tests/run-tests.ts` script

## Compatibility with e2e-tests/run-tests.ts

The generated test will work with `e2e-tests/run-tests.ts` because:
- Test directory is created in `e2e-tests/{test_name}/` (not starting with underscore)
- Required files are present: `run_test.sh` and `eval_run.sh`
- Scripts accept three arguments: PROBLEM_DIR, OUTPUT_DIR, TEST_DIR (as expected by run-tests.ts)
- Scripts use absolute paths (provided by run-tests.ts)

## Example Usage

Basic usage with required parameters:
```
Create test named "my_custom_test" with test_type: rounds_test
```

With optional parameters:
```
Create test named "performance_test" with:
- test_type: clarify_test
- requires_test_configs: false
```

Custom test type:
```
Create test named "custom_analysis_test" with:
- test_type: custom

run_test.sh script:

```bash
PROBLEM_DIR="$1"
OUTPUT_DIR="$2"
TEST_DIR="${3:-}"

mkdir -p "$OUTPUT_DIR"

if [ -n "$TEST_DIR" ] && [ -f "$TEST_DIR/debate-config.json" ]; then
  CONFIG="$TEST_DIR/debate-config.json"
else
  CONFIG="$PROBLEM_DIR/debate-config.json"
fi

# Custom test logic here
dialectic debate -r 3 -c "$CONFIG" -o "$OUTPUT_DIR/custom-debate.json" -p "$PROBLEM_DIR/problem.md" -v --some-flag
```

```

## Output

After successful execution:
1. A new test directory is created at `e2e-tests/{test_name}/`
2. The test contains all required files:
   - `run_test.sh` - Test execution script (uses new 3-argument pattern)
   - `eval_run.sh` - Evaluation script (uses new 3-argument pattern)
   - `debate-config.json` (optional, if `requires_test_configs` is true)
3. The test can be run with `e2e-tests/run-tests.ts`:
   ```bash
   npx ts-node e2e-tests/run-tests.ts <base_output_dir> --tests {test_name}
   ```
4. The test can be run against any problem:
   ```bash
   npx ts-node e2e-tests/run-tests.ts <base_output_dir> --tests {test_name} --problems {problem_name}
   ```

All scripts are ready to execute without modification.
